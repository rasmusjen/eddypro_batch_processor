# Reporting Guide

This document describes the structure, location, and interpretation of reports generated by the EddyPro Batch Processor.

## Overview

The batch processor generates comprehensive reports for each run, including:

- Run manifests (machine-readable JSON)
- HTML reports with interactive visualizations (for `run` executions)
- Per-scenario metrics and metadata
- Performance time series data

## Report Location

### Default Location

Reports are stored in a `reports/` subdirectory within the output directory:

```
{output_dir_pattern}/{site_id}/{year}/reports/
```

**Example:**
```
D:/L1_processed/GL-ZaF/2021/ec_rflux/reports/
```

### Custom Location

Override the default location:

**Via config:**
```yaml
reports_dir: "D:/custom/reports"
```

**Via CLI:**
```bash
eddypro-batch run --reports-dir /custom/reports
```

---

## Report Files

### run_manifest.json

**Purpose:** Machine-readable summary of the entire run

**Location:** `{reports_dir}/run_manifest.json`

**Schema:**

```json
{
  "run_id": "GL-ZaF_20251002_100530",
  "timestamp": "2025-10-02T10:05:30",
  "start_time": "2025-10-02T10:05:30",
  "end_time": "2025-10-02T11:20:15",
  "duration_seconds": 4485.2,
  "site_id": "GL-ZaF",
  "years_processed": [2021, 2022],
  "config_checksum": "a1b2c3d4",
  "config_snapshot": {"...": "..."},
  "overall_success": true,
  "scenarios": [
    {
      "scenario_name": "baseline",
      "scenario_params": {},
      "start_time": "2025-10-02T10:05:30",
      "end_time": "2025-10-02T11:20:15",
      "duration_seconds": 4485.2,
      "success": true
    }
  ],
  "output_dirs": ["/path/to/output"],
  "output_files": {
    "/path/to/output": {
      "fluxnet_files": [],
      "full_output_files": [],
      "metadata_files": [],
      "qc_details_files": []
    }
  },
  "environment": {
    "python_version": "3.12.6",
    "platform": "Windows-10-10.0.22631-SP0",
    "processor": "...",
    "package_versions": {
      "PyYAML": "6.0.2",
      "psutil": "5.9.8",
      "plotly": "5.22.0"
    }
  },
  "dry_run": false
}
```

---

### run_report.html

**Purpose:** Human-readable report with visualizations

**Location:** `{reports_dir}/run_report.html`

**Contents:**

1. **Run Summary**
  - Run ID, timestamps, duration
  - Site ID and years processed
  - Overall success status

2. **Scenario Results Table**
  - Scenario name and parameters
  - Duration and success status

3. **Performance Charts (Plotly only)**
  - CPU, memory, and disk I/O time series per loaded metrics file

4. **Environment**
  - Python, platform, and package versions

**Chart Engine:**
- Default: Plotly (interactive, zoomable, exportable)
- Fallback: SVG (static images)
- Override: `--report-charts svg` or `--report-charts none`

---

### Per-Scenario Artifacts

Each scenario produces additional files in its output directory:

- `scenario_manifest{suffix}.json` – scenario metadata
- `metrics{suffix}.csv` – performance time series (CPU, memory, I/O)
- `metrics_summary{suffix}.json` – summary statistics from the monitor

#### scenario_manifest{suffix}.json

**Location:** `{output_dir}/scenario{scenario_suffix}/scenario_manifest{scenario_suffix}.json`

**Example:**
```
D:/L1_processed/GL-ZaF/2021/ec_rflux/scenario_rot1_tlag2/scenario_manifest_rot1_tlag2.json
```

**Schema:**

```json
{
  "scenario_index": 1,
  "scenario_suffix": "_rot1_tlag2",
  "scenario_params": {
    "rot_meth": 1,
    "tlag_meth": 2
  },
  "project_file": "/path/to/GL-ZaF.eddypro",
  "output_dir": "/path/to/scenario_rot1_tlag2",
  "start_time": "2025-10-02T10:00:00",
  "end_time": "2025-10-02T10:15:32",
  "duration_seconds": 932.1,
  "success": true,
  "return_code": 0,
  "dry_run": false
}
```

#### metrics{suffix}.csv

**Location:** `{output_dir}/scenario{scenario_suffix}/metrics{suffix}.csv`

**Purpose:** Performance time series sampled during EddyPro execution

**Schema:**

```csv
timestamp,relative_time,system_cpu_percent,system_memory_total,system_memory_available,system_memory_percent,system_disk_read_bytes,system_disk_write_bytes,system_disk_read_count,system_disk_write_count,process_cpu_percent,process_memory_rss,process_memory_vms,process_memory_percent,process_io_read_bytes,process_io_write_bytes,process_io_read_count,process_io_write_count
1738480800.0,0.0,5.2,34359738368,27892195328,18.8,0,0,0,0,1.2,12320768,45154304,0.1,0,0,0,0
1738480800.5,0.5,42.1,34359738368,27179175936,20.9,1056768,262144,128,32,35.7,73400320,157286400,0.6,1048576,262144,16,4
1738480801.0,1.0,55.8,34359738368,26843545600,21.9,1572864,524288,196,64,48.2,125829120,268435456,1.0,2097152,524288,32,8
...
```

**Columns:**
- `timestamp`: Unix epoch seconds
- `relative_time`: seconds since monitoring started
- `system_cpu_percent`: system-wide CPU usage
- `system_memory_total`, `system_memory_available`, `system_memory_percent`
- `system_disk_read_bytes`, `system_disk_write_bytes`, `system_disk_read_count`, `system_disk_write_count`
- `process_cpu_percent`, `process_memory_rss`, `process_memory_vms`, `process_memory_percent`
- `process_io_read_bytes`, `process_io_write_bytes`, `process_io_read_count`, `process_io_write_count`

Column presence can vary by platform and psutil capabilities. The CSV is a raw
time series; summary statistics are written to `metrics_summary{suffix}.json`.

---

## Interpreting Reports

### Run Summary

**Key Metrics:**

- **Duration**: Total wall-clock time for all scenarios
- **Success Rate**: `successful / total_scenarios`
- **Throughput**: Scenarios processed per hour

**Example:**
```
Run Duration: 1h 14m 45s
Scenarios: 4 total, 4 successful, 0 failed
Throughput: 3.2 scenarios/hour
```

### Scenario Comparison

**Useful for:**
- Identifying slowest/fastest parameter combinations
- Detecting memory or I/O bottlenecks
- Comparing resource usage across methods

**Look for:**
- Significantly longer durations (>2× median)
- High peak memory usage (risk of OOM)
- Low CPU usage (I/O bound or throttled)

### Performance Charts

#### CPU Usage

**Interpretation:**
- **High avg (>80%)**: CPU-bound, good utilization
- **Low avg (<30%)**: I/O-bound or waiting on disk/network
- **Spiky**: intermittent processing phases

**Actions:**
- High CPU: consider reducing `--max-proc` if oversubscribed
- Low CPU: check disk/network throughput, investigate I/O bottlenecks

#### Memory Usage

**Interpretation:**
- **Steady increase**: potential memory leak
- **Spikes at intervals**: batch processing phases (expected)
- **Near system limit**: risk of swapping or OOM

**Actions:**
- High memory: reduce scenario concurrency, increase RAM, investigate data loading

#### Disk I/O

**Interpretation:**
- **High read throughput**: reading large raw files (expected)
- **High write throughput**: writing outputs (expected)
- **Low throughput with high CPU wait**: disk bottleneck

**Actions:**
- Disk bottleneck: use faster storage (SSD/NVMe), reduce concurrent scenarios

---

## Provenance & Reproducibility

### Config Hash

**Purpose:** Unique identifier for configuration state

**Generation:** SHA256 hash of config file content (excluding comments/whitespace)

**Use:**
- Compare runs to detect config changes
- Link outputs to exact configuration used

### Git SHA

**Purpose:** Link reports to source code version

**Captured:** Git commit hash at runtime (if repository available)

**Use:**
- Reproduce results with specific code version
- Track code changes between runs

### Environment Snapshot

**Captured:**
- Python version
- Package versions (psutil, plotly, yaml, etc.)
- EddyPro version (if detectable)

**Use:**
- Reproduce results with identical environment
- Diagnose version-specific issues

---

## Exporting & Archiving

### JSON Exports

Machine-readable manifests can be:
- Ingested by downstream analysis tools
- Archived in databases
- Compared programmatically across runs

**Example workflow:**
```bash
# Extract scenario durations from manifest
jq '.scenarios[] | {id: .scenario_id, duration: .duration_seconds}' run_manifest.json
```

### HTML Reports

- Self-contained (embedded CSS/JS)
- Portable (share via email, intranet)
- Printable (with charts)

**Archiving:**
```bash
# Archive reports with timestamp
tar -czf reports_GL-ZaF_2021_$(date +%Y%m%d).tar.gz reports/
```

---

## Customizing Reports

### Chart Engine

**Plotly (default):**
- Interactive (zoom, pan, hover tooltips)
- Export to PNG/SVG/PDF
- Requires `plotly` package

**SVG:**
- Static images
- Lightweight
- No dependencies

**None:**
- Text and tables only
- Fastest generation
- Minimal file size

**Select:**
```bash
eddypro-batch run --report-charts svg
```

### Metrics Sampling Interval

**Higher resolution (more detail):**
```bash
eddypro-batch run --metrics-interval 0.1  # Sample every 100ms
```

**Lower resolution (less overhead):**
```bash
eddypro-batch run --metrics-interval 2.0  # Sample every 2 seconds
```

**Trade-offs:**
- Lower interval: more data, higher overhead, larger files
- Higher interval: less data, lower overhead, smaller files

---

## Troubleshooting

### Missing Reports

**Symptom:** Reports directory empty after run

**Possible Causes:**
- Run failed before report generation
- Insufficient disk space
- Permission issues

**Solutions:**
- Check logs for errors
- Verify `reports_dir` permissions
- Ensure adequate disk space

### Broken Charts

**Symptom:** HTML report displays chart placeholders or errors

**Possible Causes:**
- Plotly not installed
- Incompatible Plotly version
- Browser JavaScript disabled

**Solutions:**
- Install Plotly: `pip install plotly`
- Use SVG fallback: `--report-charts svg`
- Update browser or enable JavaScript

### Large Metrics Files

**Symptom:** `metrics.csv` files are very large

**Cause:** High sampling frequency and long scenario durations

**Solutions:**
- Increase `--metrics-interval` (e.g., from 0.5 to 1.0 or 2.0)
- Compress old metrics files: `gzip metrics.csv`
- Implement metrics retention policy (delete after N days)

---

## See Also

- [USAGE.md](USAGE.md) – CLI usage and examples
- [SCENARIOS.md](SCENARIOS.md) – Scenario matrix runs
- [CONFIG.md](CONFIG.md) – Configuration options
