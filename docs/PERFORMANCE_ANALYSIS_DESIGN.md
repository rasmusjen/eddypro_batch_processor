# Performance Analysis & Reporting Design Proposal

## 1. Current State Analysis

Based on the existing `run_manifest.json` and `metrics.csv` generated by the batch processor, we already collect the raw data necessary for identifying bottlenecks.

### Available Metrics
The system currently collects (via `monitor.py`):
- **CPU**: Process %, System %
- **Memory**: RSS (Physical), VMS (Virtual)
- **Disk I/O**: Read/Write (MB), Read/Write (IOPS)
- **Time**: Duration per scenario

### The Missing Piece
What is currently missing is **contextual interpretation**. We have raw numbers, but they don't immediately tell a user if the run was "slow due to disk speed" or "slow due to CPU saturation."

---

## 2. Bottleneck Classification Logic

We can determine the primary bottleneck by comparing metrics against system limits or defined thresholds.

| Bottleneck Type | Indicator | Threshold (Traffic Light) |
|:---|:---|:---|
| **CPU Bound** | High CPU utilization sustained over time. | ðŸ”´ **Red**: > 90% avg utilization<br>ðŸŸ¡ **Yellow**: 70-90% avg utilization<br>ðŸŸ¢ **Green**: < 70% avg utilization |
| **Memory Bound** | High RAM usage close to system limits, or excessive swapping. | ðŸ”´ **Red**: > 85% of physical RAM<br>ðŸŸ¡ **Yellow**: 70-85% of physical RAM<br>ðŸŸ¢ **Green**: < 70% of physical RAM |
| **I/O Bound (Throughput)** | Low CPU usage paired with high Disk MB/s. | ðŸ”´ **Red**: CPU < 20% AND Disk I/O > 80% of drive capability*<br>ðŸŸ¡ **Yellow**: CPU < 40% AND moderate I/O<br>ðŸŸ¢ **Green**: I/O does not correlate with low CPU |
| **I/O Bound (Latency/IOPS)** | Low CPU usage, low throughput, but high IOPS (many small files). | ðŸ”´ **Red**: CPU < 20% AND IOPS > 1000 (typical HDD limit) or High Wait Time<br>ðŸŸ¡ **Yellow**: CPU < 40% AND moderate IOPS |

*\*Note: Drive capability requires either user config or a benchmark.*

---

## 3. Proposed Report Design (Mockups)

### A. Executive Summary (The "Traffic Light" Dashboard)

A new section at the top of the HTML report.

```markdown
Run ID: 20250202_GL-Dsk_2025
Duration: 1h 45m
Overall Status: âœ… Success

**Performance Health Check:**

| Resource | Status | Limiting Factor? | Analysis |
|:---|:---:|:---:|:---|
| **Processor (CPU)** | ðŸ”´ | **YES** | **High Saturation.** Average usage 94%. We recommend reducing `max_processes` or moving to a larger machine. |
| **Memory (RAM)** | ðŸŸ¢ | No | Peak usage 4.2GB (13% of System RAM). Plenty of headroom. |
| **Storage (I/O)** | ðŸŸ¡ | Potential | Moderate read latency detected during "Raw Data Processing" phase. |
```

### B. Time-Series Visualization (Interactive Graphs)

Using Plotly (which is already integrated), we should overlay the phases of EddyPro processing on top of resource usage.

**Mockup Concept:**

*   **X-Axis:** Time (HH:MM:SS)
*   **Y-Axis (Left):** CPU % (0-100)
*   **Y-Axis (Right):** Disk I/O (MB/s)

**Visual Elements:**
*   **Shaded Regions:** Background colors indicating the processing stage (e.g., "Loading Files", "Calculating Fluxes", "Spectral Analysis").
*   **Lines:**
    *   Blue Line: CPU Usage
    *   Red Line: Disk Read MB/s
    *   Green Line: Disk Write MB/s
*   **Threshold Line:** A dotted horizontal line at 90% CPU.

*(When the Blue line stays above the dotted line, the background could tint Red to visualize the bottleneck duration).*

### C. Scenario Comparison Matrix

If running multiple scenarios (e.g., `_rot1` vs `_rot3`), a heatmap allows quick identification of efficient configurations.

**Mockup:**

| Scenario | Duration | Avg CPU | Peak RAM | Bottleneck |
|:---|---:|---:|---:|:---|
| `_rot1_tlag2` | 14m 20s | 45% | 1.2 GB | DISK (Read) |
| `_rot3_tlag4` | **28m 10s** | **92%** | 1.8 GB | **CPU (Calc)** |

*Interpretation: Planar Fit (`rot3`) + Time Lag Optimization (`tlag4`) is computationally expensive.*

---

## 4. Implementation Plan

This plan breaks down the development into logical phases to be executed sequentially.

### Phase 1: Configuration & Data Structures

Goal: Allow users to define what "slow" or "heavy" means for their specific hardware.

1.  **Define Defaults**: Create a constant dictionary in `core.py` or a new `config_defaults.py` with the default thresholds defined in Section 2.
2.  **Update Config Schema**: Allow optional overrides in `config.yaml`:
    ```yaml
    performance_thresholds:
      cpu_high_percent: 90
      memory_high_percent: 85
      disk_io_high_mb: 100  # Adjust based on HDD vs SSD
    ```

### Phase 2: Analysis Logic (New Module)

Goal: Decouple analysis from reporting. Create a standardized way to digest `metrics.csv`.

1.  **Create Module**: `src/eddypro_batch_processor/analysis.py`
2.  **Implement Class**: `BottleneckAnalyzer`
    *   **Method**: `analyze_scenario(metrics_csv_path: Path) -> ScenarioAnalysis`
    *   **Logic**:
        *   Load CSV using pandas (or pure Python CSV if pandas is heavy dependency, though pandas is preferred for stats). *Decision: Keep dependency light, use pure Python or numpy if available, otherwise simple list math.*
        *   Compute Aggregates: Mean, Max, P95 (95th percentile) for CPU, RAM, Disk Read/Write.
        *   Apply Heuristics: Compare aggregates against thresholds.
        *   Determine Status: Return `RED`, `YELLOW`, or `GREEN` and the "Primary Limiting Factor".

### Phase 3: Visualization Enhancements (`report.py`)

Goal: Visualize the correlation between resources.

1.  **Refactor Charting**: Update `report.py` to use `plotly.subplots`.
    *   **Row 1**: CPU Usage (Line chart, range 0-100).
    *   **Row 2**: Memory Usage (Line chart).
    *   **Row 3**: Disk I/O (Grouped Line chart: Read vs Write).
    *   **X-Axis**: Shared time axis.
2.  **Add Executive Summary**:
    *   Inject the "Traffic Light" HTML table at the top of the report template.
    *   Use the data from `ScenarioAnalysis` to populate the table.

### Phase 4: Integration

Goal: Connect the analysis engine to the CLI workflow.

1.  **Update `cli.py`**:
    *   After `run_eddypro` completes (in `core.py` or `cli.py`), triggering the analysis is needed.
    *   Ideally, this happens during report generation.
2.  **Update `ReportGenerator`**:
    *   Accept a list of `ScenarioAnalysis` objects.
    *   Pass these objects to the Jinja2 template (or string formatter).

### Phase 5: Testing

1.  **Unit Tests**:
    *   Create `tests/test_analysis.py`.
    *   Provide mock `metrics.csv` data representing different scenarios (e.g., a "CPU bound" file, an "I/O bound" file).
    *   Assert that `BottleneckAnalyzer` correctly identifies the bottleneck.
2.  **Integration Tests**:
    *   Run a `dry-run` scenario that generates dummy metrics.
    *   Verify the HTML report contains the Performance Section.

### Task Checklist

- [ ] Define `PerformanceThresholds` TypedDict.
- [ ] Implement `src/eddypro_batch_processor/analysis.py`.
- [ ] Add unit tests for bottleneck detection.
- [ ] Update `report.py` to use subplots.
- [ ] Update `report.py` to render the summary table.
- [ ] Verify with end-to-end run.
